{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords, reuters\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "stop_words = stopwords.words(\"english\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pickle\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"imdb_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"id\",axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brass pictures movies is not a fitting word fo...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49454</th>\n",
       "      <td>Seeing as the vote average was pretty low, and...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49455</th>\n",
       "      <td>The plot had some wretched, unbelievable twist...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49456</th>\n",
       "      <td>I am amazed at how this movieand most others h...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49457</th>\n",
       "      <td>A Christmas Together actually came before my t...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49458</th>\n",
       "      <td>Working-class romantic drama from director Mar...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49459 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment\n",
       "0      Once again Mr. Costner has dragged out a movie...       neg\n",
       "1      This is an example of why the majority of acti...       neg\n",
       "2      First of all I hate those moronic rappers, who...       neg\n",
       "3      Not even the Beatles could write songs everyon...       neg\n",
       "4      Brass pictures movies is not a fitting word fo...       neg\n",
       "...                                                  ...       ...\n",
       "49454  Seeing as the vote average was pretty low, and...       pos\n",
       "49455  The plot had some wretched, unbelievable twist...       pos\n",
       "49456  I am amazed at how this movieand most others h...       pos\n",
       "49457  A Christmas Together actually came before my t...       pos\n",
       "49458  Working-class romantic drama from director Mar...       pos\n",
       "\n",
       "[49459 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neg    24765\n",
       "pos    24694\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print Vocabulary :{'once': 74, 'again': 1, 'mr': 64, 'costner': 24, 'has': 42, 'dragged': 29, 'out': 77, 'movie': 63, 'for': 36, 'far': 33, 'longer': 60, 'than': 95, 'necessary': 66, 'aside': 12, 'from': 39, 'the': 96, 'terrific': 94, 'sea': 87, 'rescue': 86, 'sequences': 88, 'of': 71, 'which': 113, 'there': 98, 'are': 8, 'very': 107, 'few': 34, 'just': 54, 'did': 27, 'not': 69, 'care': 17, 'about': 0, 'any': 5, 'characters': 19, 'most': 62, 'us': 106, 'have': 43, 'ghosts': 40, 'in': 50, 'closet': 20, 'and': 4, 'costners': 25, 'character': 18, 'realized': 84, 'early': 31, 'on': 73, 'then': 97, 'forgotten': 38, 'until': 105, 'much': 65, 'later': 59, 'by': 16, 'time': 101, 'we': 110, 'should': 89, 'really': 85, 'is': 52, 'cocky': 22, 'overconfident': 79, 'ashton': 11, 'kutcher': 57, 'problem': 83, 'he': 44, 'comes': 23, 'off': 72, 'as': 10, 'kid': 56, 'who': 114, 'thinks': 99, 'hes': 46, 'better': 15, 'anyone': 6, 'else': 32, 'around': 9, 'him': 47, 'shows': 90, 'no': 68, 'signs': 91, 'cluttered': 21, 'his': 48, 'only': 75, 'obstacle': 70, 'appears': 7, 'to': 102, 'be': 13, 'winning': 116, 'over': 78, 'finally': 35, 'when': 112, 'well': 111, 'past': 80, 'half': 41, 'way': 109, 'point': 81, 'this': 100, 'stinker': 92, 'tells': 93, 'all': 2, 'kutchers': 58, 'told': 103, 'why': 115, 'driven': 30, 'best': 14, 'with': 117, 'prior': 82, 'inkling': 51, 'or': 76, 'foreshadowing': 37, 'magic': 61, 'here': 45, 'it': 53, 'was': 108, 'could': 26, 'do': 28, 'keep': 55, 'turning': 104, 'an': 3, 'hour': 49, 'neg': 67}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------- number of words counts passing in the first text \n",
    "countvec = CountVectorizer(analyzer = 'word')\n",
    "count1 = countvec.fit(data.iloc[0])\n",
    "print(\"Print Vocabulary :\" + str(count1.vocabulary_)+ '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costners character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks hes better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutchers ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\n",
      "STEMMED SENTENCE: \n",
      "onc again mr. costner ha drag out a movi for far longer than necessari . asid from the terrif sea rescu sequenc , of which there are veri few I just did not care about ani of the charact . most of us have ghost in the closet , and costner charact are realiz earli on , and then forgotten until much later , by which time I did not care . the charact we should realli care about is a veri cocki , overconfid ashton kutcher . the problem is he come off as kid who think he better than anyon els around him and show no sign of a clutter closet . hi onli obstacl appear to be win over costner . final when we are well past the half way point of thi stinker , costner tell us all about kutcher ghost . We are told whi kutcher is driven to be the best with no prior inkl or foreshadow . No magic here , it wa all I could do to keep from turn it off an hour in . \n"
     ]
    }
   ],
   "source": [
    "#-------------------------- Stemming\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "my_lines_list = data[\"text\"]\n",
    "\n",
    "porter=PorterStemmer()\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "print(my_lines_list[0])\n",
    "print(\"STEMMED SENTENCE: \")\n",
    "x=stemSentence(my_lines_list[0])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "Once                Once                \n",
      "again               again               \n",
      "Mr.                 Mr.                 \n",
      "Costner             Costner             \n",
      "has                 ha                  \n",
      "dragged             dragged             \n",
      "out                 out                 \n",
      "a                   a                   \n",
      "movie               movie               \n",
      "for                 for                 \n",
      "far                 far                 \n",
      "longer              longer              \n",
      "than                than                \n",
      "necessary           necessary           \n",
      "Aside               Aside               \n",
      "from                from                \n",
      "the                 the                 \n",
      "terrific            terrific            \n",
      "sea                 sea                 \n",
      "rescue              rescue              \n",
      "sequences           sequence            \n",
      "of                  of                  \n",
      "which               which               \n",
      "there               there               \n",
      "are                 are                 \n",
      "very                very                \n",
      "few                 few                 \n",
      "I                   I                   \n",
      "just                just                \n",
      "did                 did                 \n",
      "not                 not                 \n",
      "care                care                \n",
      "about               about               \n",
      "any                 any                 \n",
      "of                  of                  \n",
      "the                 the                 \n",
      "characters          character           \n",
      "Most                Most                \n",
      "of                  of                  \n",
      "us                  u                   \n",
      "have                have                \n",
      "ghosts              ghost               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "closet              closet              \n",
      "and                 and                 \n",
      "Costners            Costners            \n",
      "character           character           \n",
      "are                 are                 \n",
      "realized            realized            \n",
      "early               early               \n",
      "on                  on                  \n",
      "and                 and                 \n",
      "then                then                \n",
      "forgotten           forgotten           \n",
      "until               until               \n",
      "much                much                \n",
      "later               later               \n",
      "by                  by                  \n",
      "which               which               \n",
      "time                time                \n",
      "I                   I                   \n",
      "did                 did                 \n",
      "not                 not                 \n",
      "care                care                \n",
      "The                 The                 \n",
      "character           character           \n",
      "we                  we                  \n",
      "should              should              \n",
      "really              really              \n",
      "care                care                \n",
      "about               about               \n",
      "is                  is                  \n",
      "a                   a                   \n",
      "very                very                \n",
      "cocky               cocky               \n",
      "overconfident       overconfident       \n",
      "Ashton              Ashton              \n",
      "Kutcher             Kutcher             \n",
      "The                 The                 \n",
      "problem             problem             \n",
      "is                  is                  \n",
      "he                  he                  \n",
      "comes               come                \n",
      "off                 off                 \n",
      "as                  a                   \n",
      "kid                 kid                 \n",
      "who                 who                 \n",
      "thinks              think               \n",
      "hes                 he                  \n",
      "better              better              \n",
      "than                than                \n",
      "anyone              anyone              \n",
      "else                else                \n",
      "around              around              \n",
      "him                 him                 \n",
      "and                 and                 \n",
      "shows               show                \n",
      "no                  no                  \n",
      "signs               sign                \n",
      "of                  of                  \n",
      "a                   a                   \n",
      "cluttered           cluttered           \n",
      "closet              closet              \n",
      "His                 His                 \n",
      "only                only                \n",
      "obstacle            obstacle            \n",
      "appears             appears             \n",
      "to                  to                  \n",
      "be                  be                  \n",
      "winning             winning             \n",
      "over                over                \n",
      "Costner             Costner             \n",
      "Finally             Finally             \n",
      "when                when                \n",
      "we                  we                  \n",
      "are                 are                 \n",
      "well                well                \n",
      "past                past                \n",
      "the                 the                 \n",
      "half                half                \n",
      "way                 way                 \n",
      "point               point               \n",
      "of                  of                  \n",
      "this                this                \n",
      "stinker             stinker             \n",
      "Costner             Costner             \n",
      "tells               tell                \n",
      "us                  u                   \n",
      "all                 all                 \n",
      "about               about               \n",
      "Kutchers            Kutchers            \n",
      "ghosts              ghost               \n",
      "We                  We                  \n",
      "are                 are                 \n",
      "told                told                \n",
      "why                 why                 \n",
      "Kutcher             Kutcher             \n",
      "is                  is                  \n",
      "driven              driven              \n",
      "to                  to                  \n",
      "be                  be                  \n",
      "the                 the                 \n",
      "best                best                \n",
      "with                with                \n",
      "no                  no                  \n",
      "prior               prior               \n",
      "inkling             inkling             \n",
      "or                  or                  \n",
      "foreshadowing       foreshadowing       \n",
      "No                  No                  \n",
      "magic               magic               \n",
      "here                here                \n",
      "it                  it                  \n",
      "was                 wa                  \n",
      "all                 all                 \n",
      "I                   I                   \n",
      "could               could               \n",
      "do                  do                  \n",
      "to                  to                  \n",
      "keep                keep                \n",
      "from                from                \n",
      "turning             turning             \n",
      "it                  it                  \n",
      "off                 off                 \n",
      "an                  an                  \n",
      "hour                hour                \n",
      "in                  in                  \n"
     ]
    }
   ],
   "source": [
    "#--------------------------- Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = data[\"text\"]\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence[0])\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'in', 'is', 'it', 'of', 'that', 'the', 'this', 'to', 'was']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#---------------------- Count Vectorizer\n",
    "vectorizer = CountVectorizer(analyzer = 'word',max_features=10)\n",
    "count_model = vectorizer.fit(data['text'])\n",
    "X = count_model.transform(data['text'])\n",
    "count_model.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 3,  2,  3,  2,  5,  0,  7,  1,  3,  1],\n",
       "        [ 9,  2,  3,  1,  5,  2, 14,  7,  3,  3],\n",
       "        [12,  2,  9,  1,  3,  1, 12,  2,  1,  0],\n",
       "        [15,  7,  4,  5,  5,  1, 13,  7,  7,  3],\n",
       "        [ 8,  4,  5,  2,  8,  0, 15,  3,  3,  0]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.todense()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['had',\n",
       " 'her',\n",
       " 'me',\n",
       " 'really',\n",
       " 'she',\n",
       " 'story',\n",
       " 'their',\n",
       " 'well',\n",
       " 'were',\n",
       " 'which']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer = 'word',max_features=10,max_df=0.3)\n",
    "count_model = vectorizer.fit(data['text'])\n",
    "X = count_model.transform(data['text'])\n",
    "count_model.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 1, 0, 0, 0, 1, 0, 2],\n",
       "        [0, 0, 0, 2, 0, 0, 0, 1, 0, 3],\n",
       "        [1, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 1, 2, 0],\n",
       "        [0, 2, 0, 3, 3, 1, 2, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.todense()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['had',\n",
       " 'her',\n",
       " 'me',\n",
       " 'really',\n",
       " 'she',\n",
       " 'story',\n",
       " 'their',\n",
       " 'well',\n",
       " 'were',\n",
       " 'which']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------ Tfidf Vectorizer \n",
    "vectorizer_tfidf = TfidfVectorizer(analyzer='word',max_features=10,max_df=0.3)\n",
    "tfidf_model = vectorizer_tfidf.fit(data['text'])\n",
    "X_tfidf = tfidf_model.transform(data['text'])\n",
    "tfidf_model.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.40585313, 0.        ,\n",
       "         0.        , 0.        , 0.405903  , 0.        , 0.81885652],\n",
       "        [0.        , 0.        , 0.        , 0.53150329, 0.        ,\n",
       "         0.        , 0.        , 0.2657843 , 0.        , 0.80427791],\n",
       "        [0.56860258, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.58421378, 0.        , 0.57912466, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.41338428, 0.39676709, 0.81956654, 0.        ],\n",
       "        [0.        , 0.37944341, 0.        , 0.52408471, 0.62234036,\n",
       "         0.17600976, 0.3640675 , 0.17471637, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf.todense()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.91      0.89      0.90      4890\n",
      "         pos       0.90      0.91      0.90      5002\n",
      "\n",
      "    accuracy                           0.90      9892\n",
      "   macro avg       0.90      0.90      0.90      9892\n",
      "weighted avg       0.90      0.90      0.90      9892\n",
      "\n",
      "[[4360  530]\n",
      " [ 449 4553]]\n"
     ]
    }
   ],
   "source": [
    "#------------------------ Creating Logistic Regression Full Model\n",
    "vectorizer_logtfidf = TfidfVectorizer(analyzer='word',max_features=100000)\n",
    "tfidf_model = vectorizer_logtfidf.fit(data['text'])\n",
    "X_tfidf = tfidf_model.transform(data['text'])\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_tfidf,data['sentiment'],test_size = 0.2,random_state = 0)\n",
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "clf.fit(X_train,y_train)\n",
    "predLogistic = clf.predict(X_test)\n",
    "print(classification_report(y_test,predLogistic))\n",
    "print(confusion_matrix(y_test,predLogistic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.87      0.87      0.87      4890\n",
      "         pos       0.87      0.88      0.87      5002\n",
      "\n",
      "    accuracy                           0.87      9892\n",
      "   macro avg       0.87      0.87      0.87      9892\n",
      "weighted avg       0.87      0.87      0.87      9892\n",
      "\n",
      "[[4230  660]\n",
      " [ 611 4391]]\n"
     ]
    }
   ],
   "source": [
    "#------------------------ Creating Logistic Regression Full Model with 1000 features\n",
    "vectorizer_logtfidf = TfidfVectorizer(analyzer='word',max_features=1000,max_df=0.8)\n",
    "tfidf_model = vectorizer_logtfidf.fit(data['text'])\n",
    "X_tfidf = tfidf_model.transform(data['text'])\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_tfidf,data['sentiment'],test_size = 0.2,random_state = 0)\n",
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "clf.fit(X_train,y_train)\n",
    "predLogistic = clf.predict(X_test)\n",
    "print(classification_report(y_test,predLogistic))\n",
    "print(confusion_matrix(y_test,predLogistic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.81      0.84      0.83      4890\n",
      "         pos       0.84      0.81      0.83      5002\n",
      "\n",
      "    accuracy                           0.83      9892\n",
      "   macro avg       0.83      0.83      0.83      9892\n",
      "weighted avg       0.83      0.83      0.83      9892\n",
      "\n",
      "[[4115  775]\n",
      " [ 940 4062]]\n"
     ]
    }
   ],
   "source": [
    "#---------------------- Creating Random Forest Classifier Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "vectorizer_logtfidf = TfidfVectorizer(analyzer='word',max_features=1000,max_df=0.2)\n",
    "tfidf_model = vectorizer_logtfidf.fit(data['text'])\n",
    "X_tfidf = tfidf_model.transform(data['text'])\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_tfidf,data['sentiment'],test_size = 0.2,random_state = 0)\n",
    "clf = OneVsRestClassifier(RandomForestClassifier())\n",
    "clf.fit(X_train,y_train)\n",
    "predRF = clf.predict(X_test)\n",
    "print(classification_report(y_test,predRF))\n",
    "print(confusion_matrix(y_test,predRF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.82      0.84      0.83      4890\n",
      "         pos       0.84      0.82      0.83      5002\n",
      "\n",
      "    accuracy                           0.83      9892\n",
      "   macro avg       0.83      0.83      0.83      9892\n",
      "weighted avg       0.83      0.83      0.83      9892\n",
      "\n",
      "[[4115  775]\n",
      " [ 881 4121]]\n"
     ]
    }
   ],
   "source": [
    "#---------------------- Creating Random Forest Classifier Model with stopwords\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "vectorizer_logtfidf = TfidfVectorizer(analyzer='word',max_features=1000,max_df=0.8)\n",
    "tfidf_model = vectorizer_logtfidf.fit(data['text'])\n",
    "X_tfidf = tfidf_model.transform(data['text'])\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_tfidf,data['sentiment'],test_size = 0.2,random_state = 0)\n",
    "clf = OneVsRestClassifier(RandomForestClassifier())\n",
    "clf.fit(X_train,y_train)\n",
    "predRF = clf.predict(X_test)\n",
    "print(classification_report(y_test,predRF))\n",
    "print(confusion_matrix(y_test,predRF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.84      0.90      0.87      4890\n",
      "         pos       0.90      0.83      0.86      5002\n",
      "\n",
      "    accuracy                           0.87      9892\n",
      "   macro avg       0.87      0.87      0.86      9892\n",
      "weighted avg       0.87      0.87      0.86      9892\n",
      "\n",
      "[[4405  485]\n",
      " [ 850 4152]]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- Creating Multinominal Naive Bayes Algorithm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "vectorizer_logtfidf = TfidfVectorizer(analyzer='word',max_features=1000000,max_df=0.8)\n",
    "tfidf_model = vectorizer_logtfidf.fit(data['text'])\n",
    "X_tfidf = tfidf_model.transform(data['text'])\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_tfidf,data['sentiment'],test_size = 0.2,random_state = 0)\n",
    "clf = OneVsRestClassifier(MultinomialNB())\n",
    "clf.fit(X_train,y_train)\n",
    "predRF = clf.predict(X_test)\n",
    "print(classification_report(y_test,predRF))\n",
    "print(confusion_matrix(y_test,predRF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
